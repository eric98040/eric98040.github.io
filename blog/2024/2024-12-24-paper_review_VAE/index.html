<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <p>Variational Auto-Encoders (VAEs) are powerful generative models that combine the ideas of variational inference with deep learning. Introduced by Kingma and Welling in 2013, VAEs provide a principled way to perform both inference and learning in deep latent variable models.</p> <p>The key innovation of VAEs lies in their ability to learn complex probability distributions over high-dimensional data spaces by optimizing a variational lower bound using stochastic gradient descent. This allows them to generate new samples, perform dimensionality reduction, and learn useful representations of data.</p> <h2 id="background">Background</h2> <h3 id="variational-inference">Variational Inference</h3> <table> <tbody> <tr> <td>Variational inference is a method from Bayesian statistics that approximates complex posterior distributions with simpler ones. Given a latent variable model $p(x,z)$, we want to approximate the true posterior $p(z</td> <td>x)$ with a simpler distribution $q_{\phi}(z</td> <td>x)$.</td> </tr> </tbody> </table> <h3 id="expectation-maximization-algorithm">Expectation-Maximization Algorithm</h3> <p>The EM algorithm is a classical approach for learning latent variable models. It consists of two steps:</p> <ol> <li> <p>E-step: Compute the expected complete log-likelihood with respect to the posterior: $Q(\theta|\theta^{old}) = \mathbb{E}_{p(z|x,\theta^{old})}[\log p(x,z|\theta)]$</p> </li> <li> <p>M-step: Maximize this expectation with respect to Œ∏: $\theta^{new} = \arg\max_\theta Q(\theta|\theta^{old})$</p> </li> </ol> <p>Key limitations of EM:</p> <ul> <li> <table> <tbody> <tr> <td>Requires computation of true posterior p(z</td> <td>x)</td> </tr> </tbody> </table> </li> <li>E-step often intractable for complex models</li> <li>Not directly applicable to large datasets</li> </ul> <h3 id="monte-carlo-em">Monte Carlo EM</h3> <p>Monte Carlo EM (MCEM) attempts to address intractability by using sampling:</p> <ol> <li> <p>E-step: Draw samples from posterior using MCMC: $z^{(l)} \sim p(z|x,\theta^{old})$ Approximate Q-function: $Q(\theta|\theta^{old}) \approx \frac{1}{L}\sum_{l=1}^L \log p(x,z^{(l)}|\theta)$</p> </li> <li> <p>M-step: Same as standard EM</p> </li> </ol> <p>Limitations of MCEM:</p> <ul> <li>Sampling from posterior still required</li> <li>MCMC sampling per datapoint is expensive</li> <li>Poor scaling to large datasets</li> </ul> <h3 id="auto-encoders">Auto-Encoders</h3> <p>Traditional auto-encoders learn to compress data into a lower-dimensional representation by minimizing reconstruction error. However, they lack a probabilistic interpretation and cannot generate new samples.</p> <h2 id="problem-setting-and-method">Problem Setting and Method</h2> <h3 id="problem-scenario">Problem Scenario</h3> <p>Consider a dataset X = {x^(i)}^N_{i=1} consisting of N i.i.d. samples. We assume the data are generated by a random process involving an unobserved continuous random variable z:</p> <ol> <li>A value z^(i) is generated from a prior distribution p_Œ∏*(z)</li> <li> <table> <tbody> <tr> <td>A value x^(i) is generated from a conditional distribution p_Œ∏*(x</td> <td>z)</td> </tr> </tbody> </table> </li> </ol> <p>Key assumptions:</p> <ul> <li> <table> <tbody> <tr> <td>The prior p_Œ∏(z) and likelihood p_Œ∏(x</td> <td>z) come from parametric families of distributions</td> </tr> </tbody> </table> </li> <li>Their PDFs are differentiable almost everywhere w.r.t. both Œ∏ and z</li> <li>The true parameters Œ∏* and values of latent variables z^(i) are unknown</li> </ul> <p>Important challenges addressed:</p> <ol> <li> <table> <tbody> <tr> <td> <strong>Intractability</strong>: The marginal likelihood p_Œ∏(x) = ‚à´ p_Œ∏(z)p_Œ∏(x</td> <td>z)dz is intractable</td> </tr> </tbody> </table> </li> <li> <strong>Large datasets</strong>: Batch optimization is too costly; need efficient minibatch methods</li> </ol> <p>The paper aims to solve:</p> <ol> <li>Efficient approximate ML/MAP estimation for Œ∏</li> <li>Efficient approximate posterior inference of z given x</li> <li>Efficient approximate marginal inference of x</li> </ol> <h3 id="the-variational-bound">The Variational Bound</h3> <p>For a single datapoint, the marginal likelihood can be rewritten as:</p> <table> <tbody> <tr> <td>$\log p_\theta(x^{(i)}) = D_{KL}(q_\phi(z</td> <td>x^{(i)})</td> <td>¬†</td> <td>p_\theta(z</td> <td>x^{(i)})) + \mathcal{L}(\theta, \phi; x^{(i)})$</td> </tr> </tbody> </table> <p>where:</p> <ul> <li>First term is the KL divergence between approximate and true posterior</li> <li>Second term $\mathcal{L}(\theta, \phi; x^{(i)})$ is the variational lower bound</li> </ul> <p>The lower bound can be written in two instructive ways:</p> <ol> <li> <p>As an expectation of a complete log-likelihood: $\mathcal{L}(\theta, \phi; x^{(i)}) = \mathbb{E}<em>{q</em>\phi(z|x)}[\log p_\theta(x,z) - \log q_\phi(z|x)]$</p> </li> <li> <p>As reconstruction error minus KL divergence: $\mathcal{L}(\theta, \phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)) + \mathbb{E}<em>{q</em>\phi(z|x^{(i)})}[\log p_\theta(x^{(i)}|z)]$</p> </li> </ol> <p>A key challenge is that the naive Monte Carlo gradient estimator: $\nabla_\phi \mathbb{E}<em>{q</em>\phi(z)}[f(z)] \approx \frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{q_\phi(z^{(l)})}\log q_\phi(z^{(l)})$ exhibits very high variance. This motivates the reparameterization trick.</p> <p>The VAE optimizes the evidence lower bound (ELBO):</p> \[\mathcal{L}(\theta,\phi;x) = \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z))\] <p>where:</p> <ul> <li> <table> <tbody> <tr> <td>$p_{\theta}(x</td> <td>z)$ is the decoder (generative model)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$q_{\phi}(z</td> <td>x)$ is the encoder (inference model)</td> </tr> </tbody> </table> </li> <li>$p(z)$ is the prior on the latent space</li> </ul> <h3 id="the-reparameterization-trick">The Reparameterization Trick</h3> <table> <tbody> <tr> <td>A key innovation of VAEs is the reparameterization trick, which enables backpropagation through random sampling. Instead of directly sampling from $q_{\phi}(z</td> <td>x)$, we sample from a simple distribution $\epsilon \sim \mathcal{N}(0,I)$ and transform it:</td> </tr> </tbody> </table> \[z = \mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon\] <h3 id="sgvb-estimator-and-aevb-algorithm">SGVB Estimator and AEVB Algorithm</h3> <p>The Stochastic Gradient Variational Bayes (SGVB) estimator comes in two forms:</p> <ol> <li> <p>Generic estimator: $\tilde{\mathcal{L}}^A(\theta, \phi; x^{(i)}) = \frac{1}{L}\sum_{l=1}^L \log p_\theta(x^{(i)}, z^{(i,l)}) - \log q_\phi(z^{(i,l)}|x^{(i)})$ where $z^{(i,l)} = g_\phi(\epsilon^{(l)}, x^{(i)})$ and $\epsilon^{(l)} \sim p(\epsilon)$</p> </li> <li> <p>For cases where the KL divergence is analytically tractable: $\tilde{\mathcal{L}}^B(\theta, \phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)) + \frac{1}{L}\sum_{l=1}^L \log p_\theta(x^{(i)}|z^{(i,l)})$</p> </li> </ol> <p>The complete Auto-Encoding Variational Bayes (AEVB) algorithm:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">AEVB_algorithm</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">Œ∏</span><span class="p">,</span> <span class="n">œÜ</span> <span class="o">=</span> <span class="nf">initialize_parameters</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
        <span class="c1"># Get minibatch
</span>        <span class="n">XM</span> <span class="o">=</span> <span class="nf">random_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        
        <span class="c1"># Get random samples from noise distribution
</span>        <span class="n">Œµ</span> <span class="o">=</span> <span class="nf">sample_noise</span><span class="p">(</span><span class="nf">p</span><span class="p">(</span><span class="n">Œµ</span><span class="p">))</span>
        
        <span class="c1"># Compute gradients of minibatch estimator
</span>        <span class="n">g</span> <span class="o">=</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="err">‚àá</span><span class="n">Œ∏</span><span class="p">,</span><span class="n">œÜùìõM</span><span class="p">(</span><span class="n">Œ∏</span><span class="p">,</span> <span class="n">œÜ</span><span class="p">;</span> <span class="n">XM</span><span class="p">,</span> <span class="n">Œµ</span><span class="p">))</span>
        
        <span class="c1"># Update parameters using e.g. SGD or Adagrad
</span>        <span class="n">Œ∏</span><span class="p">,</span> <span class="n">œÜ</span> <span class="o">=</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">Œ∏</span><span class="p">,</span> <span class="n">œÜ</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Œ∏</span><span class="p">,</span> <span class="n">œÜ</span>
</code></pre></div></div> <p>For Gaussian encoder and prior, the KL divergence term has the analytical solution:</p> <p>$-D_{KL} = \frac{1}{2}\sum_{j=1}^J(1 + \log((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2)$</p> <p>The Stochastic Gradient Variational Bayes (SGVB) estimator allows efficient optimization of the ELBO using stochastic gradient descent:</p> \[\mathcal{L} \approx \frac{1}{L}\sum_{l=1}^L \log p_{\theta}(x|z^{(l)}) - D_{KL}(q_{\phi}(z|x)||p(z))\] <p>where $z^{(l)}$ are samples drawn using the reparameterization trick.</p> <h2 id="architecture-and-implementation-details">Architecture and Implementation Details</h2> <h3 id="neural-network-architecture">Neural Network Architecture</h3> <p>For real-valued data, both encoder and decoder are implemented as MLPs:</p> <ol> <li>Encoder (Recognition Model) q_œÜ(z|x): <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">W3</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b3</span><span class="p">)</span>
    <span class="n">Œº</span> <span class="o">=</span> <span class="n">W4</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b4</span>
    <span class="n">log_œÉ2</span> <span class="o">=</span> <span class="n">W5</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b5</span>
    <span class="k">return</span> <span class="n">Œº</span><span class="p">,</span> <span class="n">log_œÉ2</span>
</code></pre></div> </div> </li> <li>Decoder (Generative Model) p_Œ∏(x|z): <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">W1</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">x_recon</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">W2</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>  <span class="c1"># for binary data
</span>    <span class="c1"># or for continuous data:
</span>    <span class="c1"># Œº = W2 @ h + b2
</span>    <span class="c1"># log_œÉ2 = W3 @ h + b3
</span>    <span class="k">return</span> <span class="n">x_recon</span>
</code></pre></div> </div> </li> </ol> <h3 id="approximate-posterior">Approximate Posterior</h3> <p>For continuous latent variables, we use a Gaussian approximate posterior with diagonal covariance:</p> <table> <tbody> <tr> <td>$\log q_\phi(z</td> <td>x^{(i)}) = \log \mathcal{N}(z; Œº^{(i)}, œÉ^{2(i)}I)$</td> </tr> </tbody> </table> <p>where Œº^(i) and œÉ^{(i)} are outputs of the encoding MLP.</p> <h3 id="prior">Prior</h3> <p>The prior over the latent variables is chosen to be a centered isotropic multivariate Gaussian:</p> <p>$p_\theta(z) = \mathcal{N}(z; 0, I)$</p> <p>The VAE consists of:</p> <ol> <li> <table> <tbody> <tr> <td>An encoder network that outputs parameters of $q_{\phi}(z</td> <td>x)$ (usually mean and variance)</td> </tr> </tbody> </table> </li> <li>A sampling layer implementing the reparameterization trick</li> <li>A decoder network that reconstructs the input from the latent representation</li> </ol> <h2 id="comparison-and-experimental-results">Comparison and Experimental Results</h2> <h3 id="performance-comparison">Performance Comparison</h3> <ol> <li>VAE vs Wake-Sleep: <ul> <li>Faster convergence for VAE</li> <li>Better final solutions across all latent dimensions</li> <li>More stable training dynamics</li> </ul> </li> <li>VAE vs MCEM: <ul> <li>VAE scales better to large datasets</li> <li>MCEM requires expensive sampling per datapoint</li> <li>VAE achieves better marginal likelihood estimates</li> </ul> </li> <li>Effects of Latent Space Dimensionality: <ul> <li>Higher dimensions don‚Äôt lead to overfitting</li> <li>Regularizing effect of KL term</li> <li>Smooth latent space interpolations</li> </ul> </li> </ol> <h3 id="implementation-details">Implementation Details</h3> <p>For experiments:</p> <ul> <li>500 hidden units for MNIST</li> <li>200 hidden units for Frey Face</li> <li>Minibatch size M = 100</li> <li>Single sample L = 1 per datapoint</li> <li>Adagrad optimizer</li> </ul> <h3 id="discussion">Discussion</h3> <p>VAEs combine the strengths of variational inference and deep learning, enabling both probabilistic inference and generation. Key limitations include:</p> <ul> <li>Blurry samples due to the evidence lower bound</li> <li>Difficulty with discrete latent variables</li> <li>Posterior collapse with powerful decoders</li> </ul> <p>Recent work has addressed these limitations through:</p> <ul> <li>Alternative divergence measures</li> <li>More expressive posterior approximations</li> <li>Modified training objectives</li> </ul> <h3 id="future-directions">Future Directions</h3> <ol> <li>Hierarchical Models: <ul> <li>Deep generative architectures</li> <li>Complex posterior approximations</li> </ul> </li> <li>Time-Series Models: <ul> <li>Dynamic Bayesian networks</li> <li>Sequential latent variables</li> </ul> </li> <li>Supervised Learning: <ul> <li>Latent variable classifiers</li> <li>Semi-supervised learning</li> </ul> </li> <li>Advanced Inference: <ul> <li>Flow-based posteriors</li> <li>Importance weighted bounds</li> </ul> </li> </ol> <h2 id="technical-details-and-derivations">Technical Details and Derivations</h2> <h3 id="detailed-kl-divergence-computation-for-gaussian-case">Detailed KL Divergence Computation for Gaussian Case</h3> <table> <tbody> <tr> <td>For both prior p_Œ∏(z) = N(0, I) and posterior approximation q_œÜ(z</td> <td>x) being Gaussian, the KL term can be computed analytically:</td> </tr> </tbody> </table> <ol> <li>For a univariate Gaussian:</li> </ol> <p>$\int q_\theta(z)\log p(z)dz = \int \mathcal{N}(z; \mu, \sigma^2)\log \mathcal{N}(z; 0, I)dz = -\frac{1}{2}\log(2\pi) - \frac{1}{2}(\mu^2 + \sigma^2)$</p> <ol> <li>For q_Œ∏(z):</li> </ol> <p>$\int q_\theta(z)\log q_\theta(z)dz = \int \mathcal{N}(z; \mu, \sigma^2)\log \mathcal{N}(z; \mu, \sigma^2)dz = -\frac{1}{2}\log(2\pi) - \frac{1}{2}(1 + \log \sigma^2)$</p> <ol> <li>Therefore:</li> </ol> <table> <tbody> <tr> <td>$-D_{KL}(q_\phi(z</td> <td>x)</td> <td>¬†</td> <td>p_\theta(z)) = \frac{1}{2}\sum_{j=1}^J(1 + \log((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2)$</td> </tr> </tbody> </table> <h3 id="marginal-likelihood-estimator">Marginal Likelihood Estimator</h3> <p>For low-dimensional latent spaces (&lt; 5 dimensions), we can estimate the marginal likelihood using:</p> <ol> <li>Sample L values {z^(l)} from posterior using HMC</li> <li>Fit a density estimator q(z) to these samples</li> <li>Compute the estimator:</li> </ol> <table> <tbody> <tr> <td>$p_\theta(x^{(i)}) \approx \left(\frac{1}{L}\sum_{l=1}^L \frac{q(z^{(l)})}{p_\theta(z)p_\theta(x^{(i)}</td> <td>z^{(l)})}\right)^{-1}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>where z^(l) ~ p_Œ∏(z</td> <td>x^(i))</td> </tr> </tbody> </table> <p>Derivation:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1/p_Œ∏(x^(i)) = ‚à´q(z)dz/p_Œ∏(x^(i))
               = ‚à´q(z)p_Œ∏(x^(i),z)/(p_Œ∏(x^(i),z)p_Œ∏(x^(i)))dz
               = ‚à´p_Œ∏(z|x^(i))q(z)/p_Œ∏(x^(i),z)dz
               ‚âà 1/L ‚àë(q(z^(l))/(p_Œ∏(z)p_Œ∏(x^(i)|z^(l))))
</code></pre></div></div> <h3 id="monte-carlo-em-details">Monte Carlo EM Details</h3> <p>For comparison with VAE, the MCEM procedure uses:</p> <ul> <li>10 HMC leapfrog steps</li> <li>Automatically tuned stepsize (90% acceptance rate)</li> <li>5 weight update steps per sample</li> <li>Adagrad optimizer with annealing</li> </ul> <p>The marginal likelihood estimation uses:</p> <ul> <li>First 1000 datapoints from train/test sets</li> <li>50 posterior samples per datapoint</li> <li>4 leapfrog steps for HMC</li> </ul> <h2 id="equations">Equations</h2> <p>Key equations in VAEs include:</p> <ol> <li> <p>ELBO: \(\log p(x) \geq \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z))\)</p> </li> <li> <p>Reparameterization: \(z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)\)</p> </li> <li> <p>KL divergence for Gaussian case: \(D_{KL} = \frac{1}{2}\sum_{j=1}^J(1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)\)</p> </li> </ol> <h2 id="code-blocks">Code Blocks</h2> <p>Here‚Äôs a simple PyTorch implementation of the VAE‚Äôs forward pass:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Encode
</span>    <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Reparameterize
</span>    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>
    
    <span class="c1"># Decode
</span>    <span class="n">recon_x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    
    <span class="c1"># Loss
</span>    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="p">.</span><span class="nf">exp</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">recon_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kl_loss</span>
</code></pre></div></div> <h2 id="interactive-plots">Interactive Plots</h2> <p>Visualizations of the latent space and reconstructions can be created using plotly. For example, latent space interpolations or t-SNE visualizations of the encoded representations.</p> <h2 id="references">References</h2> <p>See the original papers:</p> <ul> <li>Auto-Encoding Variational Bayes (Kingma &amp; Welling, 2013)</li> <li>Stochastic Backpropagation and Approximate Inference (Rezende et al., 2014)</li> </ul> </body></html>