---
layout: post
title: "Why I Returned to Textbooks: My Mathematical Roadmap for Deep Learning Research"
date: 2025-12-14 10:00:00 +0900
tags: [Research, Mathematics, Deep Learning, Study]
categories: [Study, Roadmap]
toc: true
---

<style>
blockquote {
    font-size: 1em !important;
    font-style: normal !important;
    color: #555 !important;
}
</style>

> **üóìÔ∏è Last Updated: 2025.12.14**
> <br> *This post is a living document. I will continue to update the book list and comments as I progress through my research journey.*

---

As an undergraduate researcher aiming to dive deep into **Generative Models**, **Deep Reinforcement Learning**, and **Physics-Informed Machine Learning (PIML)**, I recently faced a critical turning point in my career.

In the early days of my research, I found myself merely skimming through the equations in papers. I would often blindly accept the authors' claims, implementing their models without truly understanding the theoretical guarantees behind them. I realized I lacked the **"critical eye"** necessary to judge whether a methodology was rigorously sound or if the experiments logically supported the arguments.

To transition from a practitioner who simply "uses" models to a researcher who **constructs novel logical frameworks** and **proposes original algorithms**, relying on intuition and code snippets was not enough. I needed a tool to rigorously validate existing ideas and build my own.

While code brings these models to life, their **theoretical foundations are deeply rooted in Mathematics.**

This post outlines the mathematical roadmap I have constructed to build that foundation. The resources listed below are those I have personally studied or am currently studying to acquire the logical strength required for a Ph.D. and beyond.

---

## Part 1: The Non-Negotiable Essentials üèóÔ∏è

The following three areas (Calculus, Linear Algebra, Statistics) are the absolute prerequisites for **any** field of AI. Without these, it is impossible to deeply understand the mechanics of model training and inference.

<br>

### 1. Calculus üìâ
Many assume Calculus in AI is just about calculating gradients for backpropagation. However, its role is far more profound. **Calculus is the language of continuous change.**

To conduct research in **Generative Models** or **Physics-Informed ML**, you must understand how functions approximate reality and how distributions transform.
* **Modeling Dynamics:** Neural ODEs and Diffusion Models treat depth/time as a continuous variable, requiring a solid grasp of differential equations.
* **Change of Variables:** In Normalizing Flows, understanding the Jacobian Determinant is essential to track how probability density changes during transformation.
* **Approximation:** Taylor Series are not just for exams; they explain how local approximations allow neural networks to learn complex non-linear functions.

<br>

#### üìö Recommended Books

**`Calculus (James Stewart)`**
> **Why this book:** This is the standard bible for undergraduate calculus. I used this to solidify my understanding of the basics: limits, derivatives, and integrals. It is crucial for ensuring there are no holes in your fundamental calculation skills before moving to vector spaces.
>
> [üëâ Link to Book](https://www.amazon.com/Calculus-James-Stewart/dp/1285740629)

<br>

**`Vector Calculus (J. Marsden & A. Tromba)`**
> **Why this book:** Neural networks operate in high-dimensional vector spaces. This book provides the rigor needed to understand gradients, Jacobians, and Hessians in $$\mathbb{R}^n$$. It is essential for understanding how concepts like the "Chain Rule" expand into matrix-vector products in deep architectures.
>
> [üëâ Link to Book](https://www.amazon.com/Vector-Calculus-Jerrold-E-Marsden/dp/1429215089)

---

### 2. Linear Algebra üìê
If Calculus describes how things change, **Linear Algebra describes the structure they live in.** It is the "Container" and "Tool" for every piece of data we handle.

Every weight, bias, and latent vector in Deep Learning is represented as a tensor. Concepts like **Rank**, **Singular Value Decomposition (SVD)**, and **Eigendecomposition** are not just abstract theorems; they are practical tools for:
* **Feature Extraction:** Understanding which directions in data space contain the most information (PCA).
* **Model Compression:** Low-rank approximation techniques to reduce model size.
* **Stability Analysis:** Analyzing eigenvalues to prevent exploding gradients in Recurrent Neural Networks (RNNs).

*(Note on Book Choice: While pure mathematics texts like **Friedberg** or **Hoffman & Kunze** are excellent for abstract rigor, my research focuses on application and system design. Therefore, I chose books that prioritize **Geometric Intuition** and **System-Theoretic** views, which I find sufficient and more practical for AI research.)*

<br>

#### üìö Recommended Books

**`Introduction to Linear Algebra (Gilbert Strang)`**
> **Why this book:** Professor Strang is the master of intuition. Instead of getting lost in abstract proofs, I studied this book to visualize the "Four Fundamental Subspaces." It transforms Linear Algebra from a set of dry matrix operations into a geometric tool for understanding how data is projected and transformed.
>
> [üëâ Link to Book](https://math.mit.edu/~gs/linearalgebra/)

<br>

**`Linear System Theory and Design (Chi-Tsong Chen)`** *(Chapters 2-6)*
> **Why this book:** While Strang gives intuition, Chen provides the framework for **Dynamic Systems**. Chapters 2 through 6 are vital for understanding **State-Space Models (SSMs)** and **Control Theory**. This system-theoretic perspective is increasingly important in modern Deep Learning (e.g., Mamba, Linear Recurrent Units) and Reinforcement Learning.
>
> [üëâ Link to Book](https://www.amazon.com/Linear-System-Theory-Design-Chi-Tsong/dp/0199959579)

---

### 3. Mathematical Statistics üìä
Deep Learning is often mistaken for just "fitting curves" or "function approximation." In reality, it is **Statistical Inference at Scale.**

To conduct serious research, you must stop viewing Loss Functions as arbitrary formulas and start seeing them as statistical estimators.
* **Loss Functions are Estimators:** Understanding **Maximum Likelihood Estimation (MLE)** reveals that minimizing Cross-Entropy Loss is mathematically equivalent to maximizing the likelihood of the data.
* **Regularization is Bayesian Inference:** "Weight Decay" isn't just a hack to prevent overfitting; it is **Maximum A Posteriori (MAP)** estimation with a Gaussian Prior on the weights.
* **Generative Modeling:** In VAEs and Diffusion Models, we are not just generating pixels; we are explicitly modeling and sampling from complex, high-dimensional **Probability Distributions**. You need to be comfortable with concepts like the Change of Variables formula and Kullback-Leibler (KL) Divergence.

<br>

#### üìö Recommended Books

**`Mathematical Statistics with Applications (John E. Freund)`**
> **Why this book:** This is the perfect entry point.
> * Unlike other dense statistics textbooks, Freund‚Äôs book is incredibly **friendly and readable**. It avoids overwhelming the reader with excessive formality right from the start, making it ideal for beginners.
> * It strikes a great balance between theory and application, with a sufficient number of exercises to test your intuition. I used this book to build my initial confidence in probability distributions (Gamma, Beta, Normal) and random variables before tackling more rigorous texts.
>
> [üëâ Link to Book](https://www.amazon.com/John-Freunds-Mathematical-Statistics-Applications/dp/032180709X)

<br>

**`Introduction to Mathematical Statistics (Robert V. Hogg)`**
> **Why this book:** Once you are comfortable with the basics, this book is the **standard for theoretical rigor.**
> * While it might be too challenging as a first textbook, it is indispensable for a researcher. It provides deep, rigorous proofs for concepts that introductory books often gloss over‚Äîsuch as **Sufficiency**, **Completeness**, and the **Exponential Family** of distributions.
> * Mastering this book is crucial for understanding the theoretical bounds of estimation (e.g., Cram√©r-Rao lower bound) and hypothesis testing, which are the bedrocks of evaluating whether a new generative model is theoretically valid.
>
> [üëâ Link to Book](https://www.amazon.com/Introduction-Mathematical-Statistics-Robert-Hogg/dp/0321795431)

---

## Part 2: The Researcher's Arsenal (Advanced Theory) ‚öîÔ∏è

While Part 1 is for everyone, Part 2 is for those aiming to contribute to **Deep Reinforcement Learning**, **Generative Models**, or **SPDEs**. To understand the theoretical guarantees of convergence, the behavior of infinite-width networks (Neural Tangent Kernels), or the diffusion processes in continuous time, rigorous Analysis is required.

<br>

### 4. Introductory Analysis üîç
This was the most challenging yet rewarding step in my journey. It serves as the bridge between "computing" and "mathematics," providing the language to rigorously discuss limits, continuity, and convergence.

<br>

#### üìö Recommended Books

**`Understanding Analysis (Stephen Abbott)`**
> **Why this book:** I started here to build my foundation in $$\mathbb{R}$$ space. Abbott makes intimidating concepts like sequences, series, and topology incredibly intuitive. It corrected my "hand-wavy" understanding of limits, which is critical when studying the convergence of optimization algorithms.
>
> [üëâ Link to Book](https://www.amazon.com/Understanding-Analysis-Undergraduate-Texts-Mathematics/dp/1493927116)

<br>

**`Several Real Variables (Shmuel Kantorovitz)`**
> **Why this book:** Since AI deals with high-dimensional data, I needed to extend my analysis knowledge from $$\mathbb{R}$$to$$\mathbb{R}^n$$. This book covers the differentiation and integration of multivariable functions rigorously, which is a prerequisite for understanding measure theory in higher dimensions.
>
> [üëâ Link to Book](https://www.amazon.com/Several-Real-Variables-Springer-Undergraduate/dp/3319279557)

<br>

**`Principles of Mathematical Analysis (Walter Rudin)`**
> **Why this book:** Known as "Baby Rudin," this is the ultimate sharpener. I use this to review and rigorous proof-check my understanding. It is dense and brutal, but it ensures that my mathematical logic has no flaws.
>
> [üëâ Link to Book](https://www.amazon.com/Principles-Mathematical-Analysis-International-Mathematics/dp/007054235X)

*(Note: Following this, I plan to expand into Functional Analysis and Measure Theory to deeply understand Operator Learning and SDEs.)*

---

## Part 3: Deep Learning Fundamentals üß†

With the mathematical foundation in place, we need the "Bible" to connect abstract theory to concrete algorithms.

<br>

### 5. The Reference Books

**`Probabilistic Machine Learning: An Introduction & Advanced (Kevin Murphy)`**
> **Why this book:** I do not read these books cover-to-cover like a novel; I treat them as my **ultimate encyclopedia**.
> * **The "Introduction" Volume:** This covers the standard machine learning blocks (Regression, Classification, SVMs) but from a distinctly **probabilistic perspective**. It bridges the gap between the statistics I studied and the code I write.
> * **The "Advanced" Volume:** This is the real treasure for my research interests. It contains state-of-the-art coverage of **Generative Models**, **Variational Inference**, and **Diffusion Models**. When I need to understand the exact derivation of the ELBO (Evidence Lower Bound) in VAEs or the mathematical formulation of SDEs in Score-based models, this is the first book I open.
>
> [üëâ Link to Book Site](https://probml.github.io/pml-book/)

---

### üèÉ Continually Updating...
Research is a marathon, not a sprint. The list above reflects my journey so far‚Äîfrom struggling with basics to building a rigorous foundation. I will continue to update this post with new resources on **Measure Theory**, **Functional Analysis**, **Differential Equations**, and **Convex Optimization** as I conquer them.
