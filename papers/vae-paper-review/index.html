<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Review: VAE - Auto-Encoding Variational Bayes | Jaewon Kim </title> <meta name="author" content="Jaewon Kim"> <meta name="description" content="A review of the foundational paper on Variational Autoencoders"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?4c604dda15f44d9164d880f5bee041d1"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jaewon.work/papers/vae-paper-review/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Paper Review: VAE - Auto-Encoding Variational Bayes",
            "description": "A review of the foundational paper on Variational Autoencoders",
            "published": "December 23, 2024",
            "authors": [
              
              {
                "author": "Diederik P. Kingma",
                "authorURL": "https://dpkingma.com",
                "affiliations": [
                  {
                    "name": "Machine Learning Group, Universiteit van Amsterdam",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Max Welling",
                "authorURL": "https://staff.fnwi.uva.nl/m.welling/",
                "affiliations": [
                  {
                    "name": "Machine Learning Group, Universiteit van Amsterdam",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Jaewon Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_Jaewon_Kim.pdf" target="_blank">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"> <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Paper Review: VAE - Auto-Encoding Variational Bayes</h1> <p>A review of the foundational paper on Variational Autoencoders</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#variational-inference">Variational Inference</a> </li> <li> <a href="#auto-encoders">Auto-Encoders</a> </li> </ul> <div> <a href="#method">Method</a> </div> <ul> <li> <a href="#the-variational-bound">The Variational Bound</a> </li> <li> <a href="#the-reparameterization-trick">The Reparameterization Trick</a> </li> <li> <a href="#sgvb-estimator">SGVB Estimator</a> </li> </ul> <div> <a href="#architecture">Architecture</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Variational Auto-Encoders (VAEs) are powerful generative models that combine the ideas of variational inference with deep learning. Introduced by Kingma and Welling in 2013, VAEs provide a principled way to perform both inference and learning in deep latent variable models.</p> <p>The key innovation of VAEs lies in their ability to learn complex probability distributions over high-dimensional data spaces by optimizing a variational lower bound using stochastic gradient descent. This allows them to generate new samples, perform dimensionality reduction, and learn useful representations of data.</p> <h2 id="background">Background</h2> <h3 id="variational-inference">Variational Inference</h3> <table> <tbody> <tr> <td>Variational inference is a method from Bayesian statistics that approximates complex posterior distributions with simpler ones. Given a latent variable model \(p(x,z)\), we want to approximate the true posterior $$p(z</td> <td>x)\(with a simpler distribution\)q_{\phi}(z</td> <td>x)$$.</td> </tr> </tbody> </table> <h3 id="expectation-maximization-algorithm">Expectation-Maximization Algorithm</h3> <p>The EM algorithm is a classical approach for learning latent variable models. It consists of two steps:</p> <ol> <li> <p><strong>E-step</strong>: Compute the expected complete log-likelihood with respect to the posterior: \(Q(\theta|\theta^{\text{old}}) = \mathbb{E}_{p(z|x,\theta^{\text{old}})}[\log p(x,z|\theta)]\)</p> </li> <li> <p><strong>M-step</strong>: Maximize this expectation with respect to \(\theta\): \(\theta^{\text{new}} = \arg\max_\theta Q(\theta|\theta^{\text{old}})\)</p> </li> </ol> <p><strong>Key limitations of EM</strong>:</p> <ul> <li> <table> <tbody> <tr> <td>Requires computation of true posterior $$p(z</td> <td>x)$$</td> </tr> </tbody> </table> </li> <li>E-step often intractable for complex models</li> <li>Not directly applicable to large datasets</li> </ul> <h3 id="monte-carlo-em">Monte Carlo EM</h3> <p>Monte Carlo EM (MCEM) attempts to address intractability by using sampling:</p> <ol> <li> <p><strong>E-step</strong>: Draw samples from posterior using MCMC: \(z^{(l)} \sim p(z|x,\theta^{\text{old}})\)</p> \[Q(\theta|\theta^{\text{old}}) \approx \frac{1}{L}\sum_{l=1}^L \log p(x,z^{(l)}|\theta)\] </li> <li> <p><strong>M-step</strong>: Same as standard EM</p> </li> </ol> <p><strong>Limitations of MCEM</strong>:</p> <ul> <li>Sampling from posterior still required</li> <li>MCMC sampling per datapoint is expensive</li> <li>Poor scaling to large datasets</li> </ul> <h3 id="auto-encoders">Auto-Encoders</h3> <p>Traditional auto-encoders learn to compress data into a lower-dimensional representation by minimizing reconstruction error. However, they lack a probabilistic interpretation and cannot generate new samples.</p> <h2 id="problem-setting-and-method">Problem Setting and Method</h2> <h3 id="problem-scenario">Problem Scenario</h3> <p>Consider a dataset \(X = \{x^{(i)}\}_{i=1}^N\) consisting of \(N\) i.i.d. samples. We assume the data are generated by a random process involving an unobserved continuous random variable \(z\):</p> <ol> <li>A value \(z^{(i)}\) is generated from a prior distribution \(p_{\theta^*}(z)\)</li> <li> <table> <tbody> <tr> <td>A value \(x^{(i)}\) is generated from a conditional distribution $$p_{\theta^*}(x</td> <td>z)$$</td> </tr> </tbody> </table> </li> </ol> <p><strong>Key assumptions</strong>:</p> <ul> <li> <table> <tbody> <tr> <td>The prior \(p_{\theta}(z)\) and likelihood $$p_{\theta}(x</td> <td>z)$$ come from parametric families of distributions</td> </tr> </tbody> </table> </li> <li>Their PDFs are differentiable almost everywhere with respect to both \(\theta\) and \(z\)</li> <li>The true parameters \(\theta^*\) and values of latent variables \(z^{(i)}\) are unknown</li> </ul> <p><strong>Important challenges addressed</strong>:</p> <ol> <li> <table> <tbody> <tr> <td> <strong>Intractability</strong>: The marginal likelihood $$p_\theta(x) = \int p_\theta(z)p_\theta(x</td> <td>z)dz$$ is intractable</td> </tr> </tbody> </table> </li> <li> <strong>Large datasets</strong>: Batch optimization is too costly; need efficient minibatch methods</li> </ol> <p>The paper aims to solve:</p> <ol> <li>Efficient approximate ML/MAP estimation for \(\theta\)</li> <li>Efficient approximate posterior inference of \(z\) given \(x\)</li> <li>Efficient approximate marginal inference of \(x\)</li> </ol> <h3 id="the-variational-bound">The Variational Bound</h3> <p>For a single datapoint, the marginal likelihood can be rewritten as:</p> \[\log p_\theta(x^{(i)}) = D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z|x^{(i)})) + \mathcal{L}(\theta, \phi; x^{(i)})\] <p>where:</p> <ul> <li>First term is the KL divergence between approximate and true posterior</li> <li>Second term \(\mathcal{L}(\theta, \phi; x^{(i)})\) is the variational lower bound</li> </ul> <p>The lower bound can be written in two instructive ways:</p> <ol> <li> <p>As an expectation of a complete log-likelihood: \(\mathcal{L}(\theta, \phi; x^{(i)}) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z) - \log q_\phi(z|x)]\)</p> </li> <li> <p>As reconstruction error minus KL divergence: \(\mathcal{L}(\theta, \phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)) + \mathbb{E}_{q_\phi(z|x^{(i)})}[\log p_\theta(x^{(i)}|z)]\)</p> </li> </ol> <p>A key challenge is that the naive Monte Carlo gradient estimator: \(\nabla_\phi \mathbb{E}_{q_\phi(z)}[f(z)] \approx \frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{q_\phi(z^{(l)})}\log q_\phi(z^{(l)})\) exhibits very high variance. This motivates the reparameterization trick.</p> <p>The VAE optimizes the evidence lower bound (ELBO):</p> \[\mathcal{L}(\theta,\phi;x) = \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z))\] <p>where:</p> <ul> <li> <table> <tbody> <tr> <td>$$p_{\theta}(x</td> <td>z)$$ is the decoder (generative model)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$$q_{\phi}(z</td> <td>x)$$ is the encoder (inference model)</td> </tr> </tbody> </table> </li> <li>\(p(z)\) is the prior on the latent space</li> </ul> <h3 id="the-reparameterization-trick">The Reparameterization Trick</h3> <table> <tbody> <tr> <td>A key innovation of VAEs is the reparameterization trick, which enables backpropagation through random sampling. Instead of directly sampling from $$q_{\phi}(z</td> <td>x)\(, we sample from a simple distribution\)\epsilon \sim \mathcal{N}(0,I)$$ and transform it:</td> </tr> </tbody> </table> \[z = \mu_{\phi}(x) + \sigma_{\phi}(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)\] <h3 id="sgvb-estimator-and-aevb-algorithm">SGVB Estimator and AEVB Algorithm</h3> <p>The Stochastic Gradient Variational Bayes (SGVB) estimator comes in two forms:</p> <ol> <li> <p><strong>Generic estimator</strong>: \(\tilde{\mathcal{L}}^A(\theta, \phi; x^{(i)}) = \frac{1}{L}\sum_{l=1}^L \log p_\theta(x^{(i)}, z^{(i,l)}) - \log q_\phi(z^{(i,l)}|x^{(i)})\)</p> <p>where \(z^{(i,l)} = g_\phi(\epsilon^{(l)}, x^{(i)})\) and \(\epsilon^{(l)} \sim p(\epsilon)\)</p> </li> <li> <p><strong>For cases where the KL divergence is analytically tractable</strong>: \(\tilde{\mathcal{L}}^B(\theta, \phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)) + \frac{1}{L}\sum_{l=1}^L \log p_\theta(x^{(i)}|z^{(i,l)})\)</p> </li> </ol> <p>The complete Auto-Encoding Variational Bayes (AEVB) algorithm:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">AEVB_algorithm</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">θ</span><span class="p">,</span> <span class="n">φ</span> <span class="o">=</span> <span class="nf">initialize_parameters</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">converged</span><span class="p">:</span>
        <span class="c1"># Get minibatch
</span>        <span class="n">XM</span> <span class="o">=</span> <span class="nf">random_minibatch</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        
        <span class="c1"># Get random samples from noise distribution
</span>        <span class="n">ε</span> <span class="o">=</span> <span class="nf">sample_noise</span><span class="p">(</span><span class="nf">p</span><span class="p">(</span><span class="n">ε</span><span class="p">))</span>
        
        <span class="c1"># Compute gradients of minibatch estimator
</span>        <span class="n">g</span> <span class="o">=</span> <span class="nf">compute_gradients</span><span class="p">(</span><span class="err">∇</span><span class="n">θ</span><span class="p">,</span><span class="n">φ𝓛M</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">φ</span><span class="p">;</span> <span class="n">XM</span><span class="p">,</span> <span class="n">ε</span><span class="p">))</span>
        
        <span class="c1"># Update parameters using e.g. SGD or Adagrad
</span>        <span class="n">θ</span><span class="p">,</span> <span class="n">φ</span> <span class="o">=</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">θ</span><span class="p">,</span> <span class="n">φ</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">θ</span><span class="p">,</span> <span class="n">φ</span>
</code></pre></div></div> <p>For Gaussian encoder and prior, the KL divergence term has the analytical solution:</p> \[-D_{KL} = \frac{1}{2}\sum_{j=1}^J(1 + \log((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2)\] <p>The Stochastic Gradient Variational Bayes (SGVB) estimator allows efficient optimization of the ELBO using stochastic gradient descent:</p> \[\mathcal{L} \approx \frac{1}{L}\sum_{l=1}^L \log p_{\theta}(x|z^{(l)}) - D_{KL}(q_{\phi}(z|x)||p(z))\] <p>where \(z^{(l)}\) are samples drawn using the reparameterization trick.</p> <h2 id="architecture-and-implementation-details">Architecture and Implementation Details</h2> <h3 id="neural-network-architecture">Neural Network Architecture</h3> <p>For real-valued data, both encoder and decoder are implemented as MLPs:</p> <ol> <li> <strong>Encoder (Recognition Model) \(q_\phi(z|x)\)</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">W3</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b3</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">W4</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b4</span>
    <span class="n">log_σ2</span> <span class="o">=</span> <span class="n">W5</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b5</span>
    <span class="k">return</span> <span class="n">μ</span><span class="p">,</span> <span class="n">log_σ2</span>
</code></pre></div> </div> </li> <li> <strong>Decoder (Generative Model) \(p_\theta(x|z)\)</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">W1</span> <span class="o">@</span> <span class="n">z</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">x_recon</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">W2</span> <span class="o">@</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>  <span class="c1"># for binary data
</span>    <span class="c1"># or for continuous data:
</span>    <span class="c1"># μ = W2 @ h + b2
</span>    <span class="c1"># log_σ2 = W3 @ h + b3
</span>    <span class="k">return</span> <span class="n">x_recon</span>
</code></pre></div> </div> </li> </ol> <h3 id="approximate-posterior">Approximate Posterior</h3> <p>For continuous latent variables, we use a Gaussian approximate posterior with diagonal covariance:</p> \[\log q_\phi(z|x^{(i)}) = \log \mathcal{N}(z; \mu^{(i)}, \sigma^{2(i)}I)\] <p>where \(\mu^{(i)}\) and \(\sigma^{(i)}\) are outputs of the encoding MLP.</p> <h3 id="prior">Prior</h3> <p>The prior over the latent variables is chosen to be a centered isotropic multivariate Gaussian:</p> \[p_\theta(z) = \mathcal{N}(z; 0, I)\] <p>The VAE consists of:</p> <ol> <li> <table> <tbody> <tr> <td>An encoder network that outputs parameters of $$q_{\phi}(z</td> <td>x)$$ (usually mean and variance)</td> </tr> </tbody> </table> </li> <li>A sampling layer implementing the reparameterization trick</li> <li>A decoder network that reconstructs the input from the latent representation</li> </ol> <h2 id="comparison-and-experimental-results">Comparison and Experimental Results</h2> <h3 id="performance-comparison">Performance Comparison</h3> <ol> <li> <strong>VAE vs Wake-Sleep</strong>: <ul> <li>Faster convergence for VAE</li> <li>Better final solutions across all latent dimensions</li> <li>More stable training dynamics</li> </ul> </li> <li> <strong>VAE vs MCEM</strong>: <ul> <li>VAE scales better to large datasets</li> <li>MCEM requires expensive sampling per datapoint</li> <li>VAE achieves better marginal likelihood estimates</li> </ul> </li> <li> <strong>Effects of Latent Space Dimensionality</strong>: <ul> <li>Higher dimensions don’t lead to overfitting</li> <li>Regularizing effect of KL term</li> <li>Smooth latent space interpolations</li> </ul> </li> </ol> <h3 id="implementation-details">Implementation Details</h3> <p>For experiments:</p> <ul> <li>500 hidden units for MNIST</li> <li>200 hidden units for Frey Face</li> <li>Minibatch size \(M = 100\)</li> <li>Single sample \(L = 1\) per datapoint</li> <li>Adagrad optimizer</li> </ul> <h3 id="discussion">Discussion</h3> <p>VAEs combine the strengths of variational inference and deep learning, enabling both probabilistic inference and generation. <strong>Key limitations</strong> include:</p> <ul> <li>Blurry samples due to the evidence lower bound</li> <li>Difficulty with discrete latent variables</li> <li>Posterior collapse with powerful decoders</li> </ul> <p>Recent work has addressed these limitations through:</p> <ul> <li>Alternative divergence measures</li> <li>More expressive posterior approximations</li> <li>Modified training objectives</li> </ul> <h3 id="future-directions">Future Directions</h3> <ol> <li> <strong>Hierarchical Models</strong>: <ul> <li>Deep generative architectures</li> <li>Complex posterior approximations</li> </ul> </li> <li> <strong>Time-Series Models</strong>: <ul> <li>Dynamic Bayesian networks</li> <li>Sequential latent variables</li> </ul> </li> <li> <strong>Supervised Learning</strong>: <ul> <li>Latent variable classifiers</li> <li>Semi-supervised learning</li> </ul> </li> <li> <strong>Advanced Inference</strong>: <ul> <li>Flow-based posteriors</li> <li>Importance weighted bounds</li> </ul> </li> </ol> <h2 id="technical-details-and-derivations">Technical Details and Derivations</h2> <h3 id="detailed-kl-divergence-computation-for-gaussian-case">Detailed KL Divergence Computation for Gaussian Case</h3> <table> <tbody> <tr> <td>For both prior \(p_\theta(z) = \mathcal{N}(0, I)\) and posterior approximation $$q_\phi(z</td> <td>x)$$ being Gaussian, the KL term can be computed analytically:</td> </tr> </tbody> </table> <ol> <li> <strong>For a univariate Gaussian</strong>:</li> </ol> \[\int q_\theta(z)\log p(z)dz = \int \mathcal{N}(z; \mu, \sigma^2)\log \mathcal{N}(z; 0, I)dz = -\frac{1}{2}\log(2\pi) - \frac{1}{2}(\mu^2 + \sigma^2)\] <ol> <li> <strong>For \(q_\theta(z)\)</strong>:</li> </ol> \[\int q_\theta(z)\log q_\theta(z)dz = \int \mathcal{N}(z; \mu, \sigma^2)\log \mathcal{N}(z; \mu, \sigma^2)dz = -\frac{1}{2}\log(2\pi) - \frac{1}{2}(1 + \log \sigma^2)\] <ol> <li> <strong>Therefore</strong>:</li> </ol> \[-D_{KL}(q_\phi(z|x)||p_\theta(z)) = \frac{1}{2}\sum_{j=1}^J(1 + \log((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2)\] <h3 id="marginal-likelihood-estimator">Marginal Likelihood Estimator</h3> <p>For low-dimensional latent spaces (\(&lt; 5\) dimensions), we can estimate the marginal likelihood using:</p> <ol> <li>Sample \(L\) values \(\{z^{(l)}\}\) from posterior using HMC</li> <li>Fit a density estimator \(q(z)\) to these samples</li> <li>Compute the estimator:</li> </ol> \[p_\theta(x^{(i)}) \approx \left(\frac{1}{L}\sum_{l=1}^L \frac{q(z^{(l)})}{p_\theta(z)p_\theta(x^{(i)}|z^{(l)})}\right)^{-1}\] <table> <tbody> <tr> <td>where $$z^{(l)} \sim p_\theta(z</td> <td>x^{(i)})$$</td> </tr> </tbody> </table> <p><strong>Derivation</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1/p_\theta(x^{(i)}) = ∫q(z)dz/p_\theta(x^{(i)})
                   = ∫q(z)p_\theta(x^{(i)},z)/(p_\theta(x^{(i)},z)p_\theta(x^{(i)}))dz
                   = ∫p_\theta(z|x^{(i)})q(z)/p_\theta(x^{(i)},z)dz
                   ≈ 1/L ∑(q(z^{(l)})/(p_\theta(z)p_\theta(x^{(i)}|z^{(l)}))
</code></pre></div></div> <h3 id="monte-carlo-em-details">Monte Carlo EM Details</h3> <p>For comparison with VAE, the MCEM procedure uses:</p> <ul> <li>10 HMC leapfrog steps</li> <li>Automatically tuned stepsize (90% acceptance rate)</li> <li>5 weight update steps per sample</li> <li>Adagrad optimizer with annealing</li> </ul> <p>The marginal likelihood estimation uses:</p> <ul> <li>First 1000 datapoints from train/test sets</li> <li>50 posterior samples per datapoint</li> <li>4 leapfrog steps for HMC</li> </ul> <h2 id="equations">Equations</h2> <p><strong>Key equations in VAEs include</strong>:</p> <ol> <li> <p><strong>ELBO</strong>: \(\log p(x) \geq \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)] - D_{KL}(q_{\phi}(z|x)||p(z))\)</p> </li> <li> <p><strong>Reparameterization</strong>: \(z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)\)</p> </li> <li> <p><strong>KL divergence for Gaussian case</strong>: \(D_{KL} = \frac{1}{2}\sum_{j=1}^J(1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2)\)</p> </li> </ol> <h2 id="code-blocks">Code Blocks</h2> <p>Here’s a simple PyTorch implementation of the VAE’s forward pass:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Encode
</span>    <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Reparameterize
</span>    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>
    
    <span class="c1"># Decode
</span>    <span class="n">recon_x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    
    <span class="c1"># Loss
</span>    <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="p">.</span><span class="nf">exp</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">recon_x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">,</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">kl_loss</span>
</code></pre></div></div> <h2 id="interactive-plots">Interactive Plots</h2> <p>Visualizations of the latent space and reconstructions can be created using Plotly. For example, latent space interpolations or t-SNE visualizations of the encoded representations.</p> <h2 id="references">References</h2> <p>See the original papers:</p> <ul> <li>Auto-Encoding Variational Bayes (Kingma &amp; Welling, 2013)</li> <li>Stochastic Backpropagation and Approximate Inference (Rezende et al., 2014) ```</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/references.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jaewon Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"A collection of my open-source contributions and personal projects. Feel free to explore and collaborate!",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"post-paper-review-vae-auto-encoding-variational-bayes",title:"Paper Review: VAE - Auto-Encoding Variational Bayes",description:"A review of the foundational paper on Variational Autoencoders",section:"Posts",handler:()=>{window.location.href="/papers/vae-paper-review/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-i-started-an-undergraudate-research-internship-at-cdal-https-sites-google-com-view-ku-cdal",title:"I started an `undergraudate research internship` at [CDAL](https://sites.google.com/view/ku-cdal)",description:"",section:"News"},{id:"news-presented-opticalgan-at-the-12th-sk-hynix-academic-conference-receiving-the-outstanding-poster-award",title:"Presented `OpticalGAN` at the 12th SK Hynix Academic Conference, receiving the `Outstanding Poster...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6A%77%6B%30%33%30%32@%6B%6F%72%65%61.%61%63.%6B%72","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/eric98040","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/consistent","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/jae.__.one","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>